# üìù Publications 

> You can also find my articles on my [Google Scholar profile](https://scholar.google.com/citations?user=VA9mUOsAAAAJ). Note: \* indicates equal contribution; \‚Ä† indicates the corresponding author.

<!-- CARD -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv</div><img src='images/paper/card_icml26.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
# Causal Autoregressive Diffusion Language Model

Junhao Ruan, Bei Li, Yongjing Yin, `Pengcheng Huang`, Xin Chen, Jingang Wang, Xunliang Cai, Tong Xiao‚Ä†, JingBo Zhu

[**üìÑPDF**](https://arxiv.org/pdf/2601.22031)

- This paper introduces Causal Autoregressive Diffusion (CARD), a framework that unifies the training efficiency of autoregressive models with the high-throughput parallel inference of diffusion models by using a strictly causal attention mask and a dynamic decoding mechanism.
</div>
</div>


<!-- Tool master -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv</div><img src='images/paper/toolmaster_acl26.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
# Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction

Xingjie Gao*, `Pengcheng Huang*`, Zhenghao Liu‚Ä†, Yukun Yan, Shuo Wang, Zulong Chen, Chen Qian, Ge Yu, Yu Gu

[**üìÑPDF**](https://arxiv.org/pdf/2601.12762)

- This paper proposes ToolMaster, a framework that enables LLMs to master complex tool usage by shifting from passive imitation of expert trajectories to an active trial-and-execution paradigm, where models learn to explore and self-correct through direct environment interaction.
</div>
</div>


<!-- sac iclr -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2026</div><img src='images/paper/sac_iclr26.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
# Autoencoding-free context compression for llms via contextual semantic anchors

Xin Liu*, Runsong Zhao*, `Pengcheng Huang`, Xinyu Liu, Junyi Xiao, Chunyang Xiao, Tong Xiao‚Ä†, Shengxiang Gao, Zhengtao Yu, Jingbo Zhu

[**üìÑPDF**](https://arxiv.org/pdf/2510.08907?)

- This paper introduces Semantic-Anchor Compression (SAC), a novel method that achieves efficient context compression for LLMs by directly selecting and aggregating information into contextual "anchor tokens," bypassing the limitations of traditional autoencoding-based training.
</div>
</div>

<!-- attn floating mdm -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv</div><img src='images/paper/attn_floating_acl26.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
# Revealing the Attention Floating Mechanism in Masked Diffusion Models

Xin Dai, `Pengcheng Huang`, Zhenghao Liu‚Ä†, Shuo Wang, Yukun Yan, Chaojun Xiao, Yu Gu, Ge Yu, Maosong Sun

[**üìÑPDF**](https://arxiv.org/pdf/2601.07894)

- This paper investigates the internal attention behaviors of Masked Diffusion Models (MDMs), identifying a unique "Attention Floating" mechanism where attention anchors dynamically shift across layers and denoising steps, which provides a mechanistic explanation for their superior in-context learning performance compared to autoregressive models.
</div>
</div>

<!-- speech grpo -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv</div><img src='images/paper/speech_grpo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
# RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF

Qing Yang, Zhenghao Liu‚Ä†, Junxin Wang, Yangfan Du, `Pengcheng Huang`, Tong Xiao

[**üìÑPDF**](https://arxiv.org/pdf/2510.14628)

- This paper proposes RLAIF-SPA, a framework that enhances the emotional expressiveness and prosodic naturalness of LLM-based speech synthesis by using Reinforcement Learning from AI Feedback to optimize semantic accuracy and fine-grained prosodic alignment.

</div>
</div>


<!-- PamramMute_nips -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2025</div><img src='images/paper/parammute_nips25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation

`Pengcheng Huang`, Zhenghao Liu‚Ä†, Yukun Yan, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong

[**üìÑPDF**](https://arxiv.org/pdf/2502.15543)

- This work presents ParamMute, a novel framework for enhancing the faithfulness of Retrieval-Augmented Generation. By identifying and suppressing unfaithfulness-associated feed-forward networks (FFNs), and incorporating a knowledge preference adaptation module, ParamMute effectively steers language models to better leverage retrieved evidence, paving the way for more reliable and trustworthy RAG systems.
</div>
</div>

<!-- rag_survey -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AI Open 2025</div><img src='images/paper/survey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Knowledge Intensive Agents

Zhenghao Liu‚Ä†, `Pengcheng Huang`, Zhipeng Xu, Xinze Li, Shuliang Liu, Chunyi Peng, Haidong Xin, Yukun Yan, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun, Yu Gu, Ge Yu

[**üìÑPDF**](https://papers.ssrn.com/sol3/Delivery.cfm/81afde5f-fbd1-4635-b582-7ac104b3322a-MECA.pdf?abstractid=5459034&mirid=1)

- This work provides a comprehensive overview of Retrieval-Augmented Generation from an agentic perspective, categorizing knowledge-intensive agents into acquisition and utilization roles, and highlighting future directions for joint optimization in multi-agent RAG systems.
</div>
</div>


<!-- PC_sampler_aaai -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv</div><img src='images/paper/Pc_sampler_aaai26.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Pc-sampler: Position-aware calibration of decoding bias in masked diffusion models

`Pengcheng Huang`, Tianming Liu, Zhenghao Liu‚Ä†, Yukun Yan, Shuo Wang, Tong Xiao, Zulong Chen, Maosong Sun

[**üìÑPDF**](https://arxiv.org/abs/2508.13021)

- This work discovered that mainstream uncertainty‚Äëbased decoding in Masked Diffusion Models suffers from both a lack of global trajectory control and a strong early bias toward trivial tokens, and PC‚ÄëSampler remedies this by using a position‚Äëaware pathway planner together with a confidence calibration mechanism to boost informativeness, delivering over 10% gains on average while significantly narrowing the performance gap with autoregressive models.
</div>
</div>

<!-- Expandr_EMNLP_oral -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025 Oral</div><img src='images/paper/expandr_emnlp25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# ExpandR: Teaching Dense Retrievers Beyond Queries with LLM Guidance

Sijia Yao*, `Pengcheng Huang*`, Zhenghao Liu‚Ä†, Yu Gu, Yukun Yan, Shi Yu, Ge Yu


[**üìÑPDF**](https://arxiv.org/abs/2502.17057)

- This work introduces ExpandR, a unified framework that jointly trains a large language model (LLM) and a dense retriever by having the LLM generate rich query expansions and optimizing both expansion generation (via Direct Preference Optimization with combined rewards for retrieval utility and consistency) and retriever ranking performance at the same time, resulting in >5% boost over strong baselines on retrieval benchmarks.
</div>
</div>


<!-- ClueAnchor_EMNLP -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025</div><img src='images/paper/clueanchor_emnlp25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation

Hao Chen, Yukun Yan, Sen Mei, Wanxiang Che, Zhenghao Liu, Qi Shi, Xinze Li, Yuchun Fan, `Pengcheng Huang`, Qiushi Xiong, Zhiyuan Liu, Maosong Sun

[**üìÑPDF**](https://arxiv.org/abs/2505.24388)

- This work discovered that RAG systems often under‚Äêutilize retrieved documents because critical evidence (‚Äúclues‚Äù) is implicit, dispersed, or obscured by noise, and ClueAnchor addresses this by first extracting key clues from the retrieved content, generating multiple reasoning paths under different knowledge configurations (internal, external, and clue‚Äëanchored), and then using reward‚Äëbased preference optimization to select the most effective path ‚Äî yielding much better reasoning completeness and robustness.
</div>
</div>


<!-- positionid_EMNLP -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025</div><img src='images/paper/position_id_emnlp25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models

Runsong Zhao, Xin Liu, Xinyu Liu, `Pengcheng Huang`, Chunyang Xiao, Tong Xiao‚Ä†, Jingbo Zhu

[**üìÑPDF**](https://arxiv.org/abs/2409.14364)

- This work identifies that existing LLM context‚Äëcompression methods like ICAE suffer from suboptimal position identifier layouts for compressed tokens and that the auto‚Äëencoding loss alone insufficiently promotes memorization; it then proposes spreading position identifiers uniformly across input and introducing a ‚Äúcompression loss‚Äù focused purely on memorization, enabling much higher compression ratios (up to ~15√ó vs 4√ó) while retaining reconstruction and downstream performance.
</div>
</div>

<!-- Forgetting_curve_EMNLP -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2024</div><img src='images/paper/forget_emnlp24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Forgetting curve: A reliable method for evaluating memorization capability for long-context models

Xinyu Liu, Runsong Zhao, `Pengcheng Huang`, Chunyang Xiao, Bei Li, Jingang Wang, Tong Xiao‚Ä†, Jingbo Zhu

[**üìÑPDF**](https://arxiv.org/abs/2410.04727)

- This work discovered that existing methods for measuring memorization in long‚Äëcontext models (e.g. via perplexity or ‚Äúneedle in a haystack‚Äù tasks) are unreliable‚Äîbeing overly sensitive to prompts, corpora, or downstream tasks‚Äîand proposes the Forgetting Curve, a prompt‚Äëfree, robust metric based on comparing copy accuracy vs. LM accuracy over prefixes to more faithfully capture a model‚Äôs memory over long contexts, revealing that many claimed context extension techniques may not truly improve memorization for very long inputs.
</div>
</div>


<!-- TAR_CCL -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CCL 2025</div><img src='images/paper/tar_ccl24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Translate-and-Revise: Boosting Large Language Models for Constrained Translation

`Pengcheng Huang`, Yongyu Mu, Yuzhang Wu, Bei Li, Chunyang Xiao, Tong Xiao‚Ä†, Jingbo Zhu

[**üìÑPDF**](https://aclanthology.org/2024.ccl-1.82/)

- This work discovers that large language models (LLMs) prompted for constrained translation often ignore or violate specified lexical or structural constraints‚Äîpartly because they‚Äôre overly confident and let their own fluency priorities override constraint satisfaction‚Äîand Translate‚Äëand‚ÄëRevise remedies this by adding a revision process that prompts the model to identify unmet constraints and correct its output, yielding ~15% improvement in constraint translation accuracy and surpassing NMT baselines.
</div>
</div>